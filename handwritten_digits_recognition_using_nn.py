# -*- coding: utf-8 -*-
"""handwritten digits recognition using NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eKS-caZUCBNMih7k5xBFM_U9gMiedQ0g
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()

len(X_train)

len(X_test)

X_train.shape#60000 images with each one 28 x 28 pixel

X_train[0]

plt.matshow(X_train[0])#matshow means matrix show

y_train[0]

X_train = X_train / 255
X_test = X_test / 255

X_train[0]

#now lets flatten the training dataset
X_train_flattened = X_train.reshape(len(X_train), 28*28)
X_test_flattened = X_test.reshape(len(X_test), 28*28)

X_train_flattened.shape

X_train_flattened[0]
#now we can see that the 2d array is converted to 1d array

"""<h3 style='color:purple'>Very simple neural network with no hidden layers</h3>

<img src="digits_nn.jpg" />
"""

#here, sequential means that there are a stack of layers in NN
model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_flattened, y_train, epochs=5)

model.evaluate(X_test_flattened, y_test)

y_predicted = model.predict(X_test_flattened)
y_predicted[0]

plt.matshow(X_test[0])#the graph shows that it is 7

"""**np.argmax finds a maximum element  from an array and returns the index of it**"""

np.argmax(y_predicted[0])

#here, let us convert y_predicted to concrete class labels
y_predicted_labels = [np.argmax(i) for i in y_predicted]

y_predicted_labels[:5]

cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)
cm

import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')
#here the diagonal elements are correctly predicted

"""#here you can see when I add hidden layers, the accuracy increases"""

model = keras.Sequential([
    keras.layers.Dense(100, input_shape=(784,), activation='relu'), #here 100 is randomly chosen. It should be less than input size,i.e, here it should be less than 784
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_flattened, y_train, epochs=5)

model.evaluate(X_test_flattened,y_test)

y_predicted = model.predict(X_test_flattened)
y_predicted_labels = [np.argmax(i) for i in y_predicted]
cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)

plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True,fmt='d')#here annot=True is written to write the datas to each cell and fmt='d'  allows to add string (text) values on the cell
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""instead of using reshape to flatten, we can add a flatten layer as shown below"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

model.evaluate(X_test,y_test)

"""#Now let us work with different loss functions and sigmoid activation function

Here, it shows that mean squared error gives less accuracy
Computes the mean of squares of errors between labels and predictions.

loss = square(y_test - y_pred)
"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.MeanSquaredError(),
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

"""here, poisson loss gives less accuracy
It computes as following:

loss = y_pred - y_test * log(y_pred)
"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.Poisson(),
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

"""here we use mean absolute error labels and predictions.

loss = abs(y_test - y_pred)
"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.MeanAbsoluteError(),
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

